# Linear Reg 1-3

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

data = pd.read_csv('../advertising.csv')

print("\nDescribing the data:")
print(data.describe())

print("\nFew Rows:")
print(data.head())

print("\nFinding Missing Values:")
print(data.isnull().sum())

print("\nCleaning:")

# More efficient cleaning
data.fillna({
    "TV": data['TV'].mean(),
    "Radio": data['Radio'].mean(),
    "Newspaper": data['Newspaper'].mean(),
    "Sales": data['Sales'].mean()
}, inplace=True)

data.drop_duplicates(inplace=True)

print("\nAfter Cleaning:")
print(data.info())

X = data[['Radio']].values
y = data['Sales'].values

# Store mean and std for later use in prediction
X_mean = data['Radio'].mean()
X_std = data['Radio'].std()

# Standardize the features
X = (X - X_mean) / X_std

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def linear_regression(X, y, alpha=0.01, epochs=1000):
    m, n = X.shape
    W = np.zeros(n)
    b = 0

    for _ in range(epochs):
        y_pred = np.dot(X, W) + b
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum((y_pred - y))
        W = W - alpha * dw
        b = b - alpha * db

    return W, b

weights, bias = linear_regression(X_train, y_train, 0.01, 1000)

# Make predictions on test set
y_pred = np.dot(X_test, weights) + bias

# Calculate metrics
# mse = np.mean((y_pred - y_test)**2)
# rmse = np.sqrt(mse)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"\nModel Performance:")
print(f"Weight: {weights}")
print(f"Bias: {bias:.4f}")
print(f"Mean Squared Error: {mse:.4f}")
print(f"Root Mean Squared Error: {rmse:.4f}")
print(f"R² Score: {r2:.4f}")

# FIX: Sort the test data for proper line plotting
sorted_indices = np.argsort(X_test[:, 0])
X_test_sorted = X_test[sorted_indices]
y_pred_sorted = y_pred[sorted_indices]

plt.scatter(X_test, y_test, color="red", label="Actual Values", alpha=0.7)
plt.plot(X_test_sorted, y_pred_sorted, color="blue", linewidth=2, label="Predicted Line")
plt.xlabel('Radio (Standardized)')
plt.ylabel('Sales')
plt.title('Sales vs Radio Advertisement')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Predict on custom input
custom_test = float(input("\nEnter Radio Value: "))

# Apply the same standardization to custom input
custom_test_standardized = (custom_test - X_mean) / X_std

# Reshape to 2D for consistency
custom_test_standardized = np.array([[custom_test_standardized]]) # 1d array

custom_output = np.dot(custom_test_standardized, weights) + bias

print(f"Predicted Sales for Radio={custom_test}: {custom_output}")





# Multilinear Regression 4-6

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

data = pd.read_csv('../advertising.csv')

print("\nDescribing the data:")
print(data.describe())

print("\nFew Rows:")
print(data.head())

print("\nFinding Missing Values:")
print(data.isnull().sum())

print("\nCleaning:")

# Store mean and std for later use in prediction
X_mean = data[['Radio', 'TV']].values.mean(axis=0)
X_std = data[['Radio', 'TV']].values.std(axis=0)

# More efficient cleaning
data.fillna({
    "TV": data['TV'].mean(),
    "Radio": data['Radio'].mean(),
    "Newspaper": data['Newspaper'].mean(),
    "Sales": data['Sales'].mean()
}, inplace=True)

data.drop_duplicates(inplace=True)

print("\nAfter Cleaning:")
print(data.info())

X = data[['Radio', 'TV']].values
y = data['Sales'].values

# Standardize the features
X = (X - X_mean) / X_std

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def linear_regression(X, y, alpha=0.01, epochs=1000):
    m, n = X.shape
    W = np.zeros(n)
    b = 0

    for _ in range(epochs):
        y_pred = np.dot(X, W) + b
        dw = (1 / m) * np.dot(X.T, (y_pred - y))
        db = (1 / m) * np.sum((y_pred - y))
        W = W - alpha * dw
        b = b - alpha * db

    return W, b

weights, bias = linear_regression(X_train, y_train, 0.01, 1000)

# Make predictions on test set
y_pred = np.dot(X_test, weights) + bias

# Calculate metrics
# mse = np.mean((y_pred - y_test)**2)
# rmse = np.sqrt(mse)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"\nModel Performance:")
print(f"Weight: {weights}")
print(f"Bias: {bias:.4f}")
print(f"Mean Squared Error: {mse:.4f}")
print(f"Root Mean Squared Error: {rmse:.4f}")
print(f"R² Score: {r2:.4f}")

# Predict on custom input
custom_radio_test = float(input("\nEnter Radio Value: "))
custom_tv_test = float(input("\nEnter TV Value: "))

# Apply the same standardization to custom input
custom_input = np.array([[custom_radio_test, custom_tv_test]]) # 2d array
custom_input_standardized = (custom_input - X_mean) / X_std

custom_output = np.dot(custom_input_standardized, weights) + bias

print(f"Predicted Sales for Radio={custom_radio_test} and TV={custom_tv_test}: {custom_output[0]:.4f}")






# Regression Car 7-18


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

data = pd.read_csv('../car.csv')

print("\nDescribe")
print(data.describe())

print("\nFive Rows")
print(data.head())

print("\n Missing value and Null")
print(data.isnull().sum())

print("\nFilling missing values")

data.drop_duplicates(inplace=True)

num_cols = ['year_bought', 'km_driven', 'selling_price']

for col in num_cols:
    data[col].fillna(data[col].median(), inplace=True)

cat_col = ['transmission', 'owner']

for col in cat_col:
    data[col].fillna(data[col].mode()[0], inplace=True)

print("\nAfter Filling Missing Values")
print(data.isnull().sum())

print("\nFew Rows")
print(data.head())

label_encoder = LabelEncoder()
data['transmission'] = label_encoder.fit_transform(data['transmission'])
data['owner'] = label_encoder.fit_transform(data['owner'])

print("\nFew Rows")
print(data.head())

X = data[['year_bought', 'km_driven', 'transmission', 'owner']].values
y = data['selling_price'].values

X_mean = data[['year_bought', 'km_driven', 'transmission', 'owner']].values.mean(axis=0)
X_std = data[['year_bought', 'km_driven', 'transmission', 'owner']].values.std(axis=0)

X = (X - X_mean) / X_std

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def linear_regression(X, y, alpha = 0.01, epochs = 1000):

    m, n = X.shape
    W = np.zeros(n)
    b = 0

    for _ in range(epochs):

        y_pred = np.dot(X, W) + b
        dw = (1/m) * np.dot(X.T, (y_pred - y))
        db = (1/m) * np.sum((y_pred - y))
        W = W - alpha * dw
        b = b - alpha * db

    return W, b

weights, bias = linear_regression(X_train, y_train, 0.01, 1000)

y_pred = np.dot(X_test, weights) + bias

mse = np.mean((y_pred - y_test)**2)
rmse = np.sqrt(mse)

print(f"RMSE: {rmse}")






# SVM Social 19-26


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score

data = pd.read_csv('../Social_Network.csv')

data.drop_duplicates(inplace=True)
data.dropna(inplace=True)

label_encoder = LabelEncoder()
data['Gender'] = label_encoder.fit_transform(data['Gender'])

X = data[['Gender', 'Age', 'EstimatedSalary']].values
y = data['Purchased']

X = (X - X.mean()) / X.std()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm = SVC(kernel='linear', random_state=42)
# svm = SVC(kernel='rbf', gamma=0.1, random_state=42)

svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)

score = confusion_matrix(y_test, y_pred)

accuracy = (score[0][0] + score[1][1]) / (score[0][0] + score[1][1] + score[0][1] + score[1][0])

print(svm.predict([[0, 18, 47154], [1, 53, 49008]]))






# SVM Iris 27-35


import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, f1_score, recall_score, accuracy_score

data = pd.read_csv('../iris.csv')

label_encoder = LabelEncoder()

data['species'] = label_encoder.fit_transform(data['species'])

X = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = data['species'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

svm = SVC(kernel='linear', random_state=42)
# svm = SVC(kernel='rbf', random_state=42, gamma=0.21)

svm.fit(X_train, y_train)

y_pred = svm.predict(X_test)

print(accuracy_score(y_test, y_pred))

inverse_label_encoded = label_encoder.inverse_transform([0, 1, 2])

print(f"Testing Custom Input: [4.9 3.0 1.4 0.2]")
result = svm.predict([[4.9, 3.0, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]])

print(f"{inverse_label_encoded[result]}")






# Decision Tree 35-37


from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data
y = iris.target

X = ( X - X.mean(axis=0) ) / X.std(axis=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

decision_tree = DecisionTreeClassifier(criterion="gini", max_depth=4, min_samples_split=2, random_state=42)
# decision_tree = DecisionTreeClassifier(criterion="entropy", max_depth=4, min_samples_split=2)
# decision_tree = DecisionTreeClassifier(criterion="log_loss", max_depth=4, min_samples_split=2)

decision_tree.fit(X_train, y_train)
y_pred = decision_tree.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"{accuracy}")







# Logistic Regression 38


import numpy as np
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

iris = load_iris()

X = iris.data
y = iris.target

X = (X - X.mean(axis=0)) / X.std(axis=0)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, alpha=0.01, epochs=1000):
    m, n = X.shape
    classes = np.unique(y)
    W = np.zeros((len(classes), n))
    b = np.zeros(len(classes))

    for i, c in enumerate(classes):
        y_bin = (y == c).astype(int)
        w = np.zeros(n)
        bias = 0

        for _ in range(epochs):
            linear = np.dot(X, w) + bias
            y_pred = sigmoid(linear)
            dw = (1/m) * np.dot(X.T, (y_pred - y_bin))
            db = (1/m) * np.sum(y_pred - y_bin)
            w -= alpha * dw
            bias -= alpha * db

        W[i] = w
        b[i] = bias

    return W, b, classes

# # Weights (classes, features) in X.shape
#
# [[-0.6380633   0.91583982 -1.04481674 -0.97361716] -> weight of class 0
#  [ 0.05768169 -0.90919961  0.17741897 -0.09469135] -> weight of class 1
#  [ 0.5054276   0.06335685  0.78256361  0.99567327]], -> weight of class 2
#
# # bias
# [-0.81536848 -0.66183225 -0.93997667],
#
# # classes
# [0 1 2]

Weights, bias, classes = logistic_regression(X_train, y_train, 0.01, 1000)

print(f"{Weights}, {bias}, {classes}")

def predict(X, W, b, classes):
    y_linear = np.dot(X, W.T) + b
    probs = sigmoid(y_linear)
    pred_indices = np.argmax(probs, axis=1)
    return classes[pred_indices]

y_pred = predict(X_test, Weights, bias, classes)

score = accuracy_score(y_test, y_pred)

print(score)








# Ensemble PCA 39-43


import pandas as pd
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA

data = pd.read_csv('../iris.csv')

label_encoder = LabelEncoder()
data['species'] = label_encoder.fit_transform(data['species'])

X = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y = data['species'].values

X = (X - X.mean()) / X.std()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42, bootstrap=True)
bagging.fit(X_train, y_train)
bagging_y_pred = bagging.predict(X_test)

random_forest_classifier = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=None)
random_forest_classifier.fit(X_train, y_train)
random_y_pred = random_forest_classifier.predict(X_test)

adaboost_classifier = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.5, random_state=42)
adaboost_classifier.fit(X_train, y_train)
ada_y_pred = adaboost_classifier.predict(X_test)

gradient_boot_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gradient_boot_classifier.fit(X_train, y_train)
gbc_y_pred = gradient_boot_classifier.predict(X_test)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y, random_state=42, test_size=0.2)

pca_random_forest = RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=None)
pca_random_forest.fit(X_pca_train, y_pca_train)
pca_y_pred = pca_random_forest.predict(X_pca_test)

print(accuracy_score(y_test, bagging_y_pred))
print(accuracy_score(y_test, random_y_pred))
print(accuracy_score(y_test, ada_y_pred))
print(accuracy_score(y_test, gbc_y_pred))
print(accuracy_score(y_pca_test, pca_y_pred))
