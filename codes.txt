SPCC CODES

1: Program to generate Three Address Code (TAC)

def is_operator(c):
    return c in "+-*/"

def precedence(op):
    if op in ('+', '-'):
        return 1
    if op in ('*', '/'):
        return 2
    return 0

def infix_to_postfix(expression):
    stack = []
    postfix = []
    tokens = expression.split()
    
    for token in tokens:
        if token.isalnum():  # operand (e.g., a, b, c, d, x1)
            postfix.append(token)
        elif is_operator(token):
            while stack and precedence(stack[-1]) >= precedence(token):
                postfix.append(stack.pop())
            stack.append(token)
        elif token == '(':
            stack.append(token)
        elif token == ')':
            while stack and stack[-1] != '(':
                postfix.append(stack.pop())
            stack.pop()  # pop '('
    
    while stack:
        postfix.append(stack.pop())
    
    return postfix

def generate_TAC(expression):
    # assuming expression is like "a = b + c * d"
    lhs, rhs = expression.split('=')
    lhs = lhs.strip()
    rhs = rhs.strip()
    
    postfix = infix_to_postfix(rhs)
    stack = []
    temp_count = 1
    tac = []
    
    for token in postfix:
        if token.isalnum():
            stack.append(token)
        elif is_operator(token):
            op2 = stack.pop()
            op1 = stack.pop()
            temp = f"t{temp_count}"
            tac.append(f"{temp} = {op1} {token} {op2}")
            stack.append(temp)
            temp_count += 1
    
    final_result = stack.pop()
    tac.append(f"{lhs} = {final_result}")
    
    return tac

# Example usage
expr = input("Enter an expression (e.g., a = b + c * d): ")
tac_code = generate_TAC(expr)

print("\nThree Address Code:")
for line in tac_code:
    print(line)






















2. Program to implement Multi-Pass Macroprocessor for the given Assembly
Language (Pass1).

import re

# Function to process macro definitions and macro calls
def process_macros(source_code):
    macros = {}  # Store macro definitions
    mnt = {}  # Macro Name Table (MNT)
    mdt = []  # Macro Definition Table (MDT)
    at = {}   # Argument Table (AT)
    
    current_macro_name = None
    current_params = []
    current_body = []
    
    # Split the source code into lines
    lines = source_code.splitlines()
    
    # Pass 1: Find macro definitions and macro calls
    for line in lines:
        line = line.strip()
        
        # Detect macro definition (MACRO <name> <params>)
        macro_def = re.match(r"MACRO\s+(\w+)\s*(.*)", line)
        if macro_def:
            # If we encounter a macro definition, save the previous macro if it exists
            if current_macro_name:
                macros[current_macro_name] = {'params': current_params, 'body': current_body}
                mnt[current_macro_name] = len(mdt)  # Add to MNT
                mdt.extend(current_body)  # Add the body of the previous macro to MDT
            
            current_macro_name = macro_def.group(1)
            current_params = macro_def.group(2).split(",") if macro_def.group(2) else []
            current_body = []
            continue
        
        # Detect end of macro definition (MEND)
        if line.startswith("MEND"):
            # Save the macro and its body to MDT
            if current_macro_name:
                macros[current_macro_name] = {'params': current_params, 'body': current_body}
                mnt[current_macro_name] = len(mdt)  # Add to MNT
                mdt.extend(current_body)  # Add the body of the current macro to MDT
                current_macro_name = None
                current_params = []
                current_body = []
            continue
        
        # If we are inside a macro definition, add the line to the body
        if current_macro_name:
            current_body.append(line)
            continue
        
        # Process macro calls (Replace with placeholders in the MDT)
        macro_call = re.match(r"(\w+)\s*(.*)", line)
        if macro_call:
            macro_name = macro_call.group(1)
            if macro_name in macros:
                params = macro_call.group(2).split(",") if macro_call.group(2) else []
                at[macro_name] = params  # Add argument mapping to AT
                # Replace macro call with placeholder
                mdt.append(f"CALL {macro_name}")
    
    # Return all tables and processed code (MDT and MNT)
    return macros, mnt, mdt, at

# Sample Assembly Code
source_code = """
MACRO ADD X, Y
    LOAD X
    ADD Y
MEND

MACRO SUB A, B
    LOAD A
    SUB B
MEND

START
    ADD 5, 3
    SUB 7, 4
END
"""

# Process the source code
macros, mnt, mdt, at = process_macros(source_code)

# Output the results of Pass 1
print("\nMacro Definition Table (MDT):")
for line in mdt:
    print(line)

print("\nMacro Name Table (MNT):")
for name, idx in mnt.items():
    print(f"{name}: {idx}")

print("\nArgument Table (AT):")
for macro, args in at.items():
    print(f"{macro}: {', '.join(args)}")












































3. Program to implement the FIRST set for the given grammar

def compute_first(symbol, grammar, first_sets):
    # If FIRST already computed, return it
    if symbol in first_sets:
        return first_sets[symbol]

    first = set()

    # If terminal, FIRST is itself
    if not symbol.isupper():
        first.add(symbol)
        return first

    for production in grammar[symbol]:
        # If production is epsilon
        if production == "ε":
            first.add("ε")
        else:
            i = 0
            while i < len(production):
                sym = production[i]

                # Get FIRST of current symbol
                sym_first = compute_first(sym, grammar, first_sets)

                # Add all except epsilon
                first.update(sym_first - {"ε"})

                # If ε not in FIRST(sym), stop
                if "ε" not in sym_first:
                    break
                i += 1

            # If ε in all symbols, add ε
            if i == len(production):
                first.add("ε")

    # Cache result
    first_sets[symbol] = first
    return first



grammar = {
    "E": ["TE'"],
    "E'": ["+TE'", "ε"],
    "T": ["FT'"],
    "T'": ["*FT'", "ε"],
    "F": ["(E)", "id"]
}

first_sets = {}

for non_terminal in grammar:
    compute_first(non_terminal, grammar, first_sets)

print("\nFIRST sets:")
for nt, first in first_sets.items():
    print(f"FIRST({nt}) = {{ {', '.join(sorted(first))} }}")




































4. Program to implement the FOLLOW set for the given grammar

def compute_follow(grammar, first_sets, non_terminals, start_symbol):

    follow_sets = {non_terminal: set() for non_terminal in grammar}
    follow_sets[start_symbol].add('$') 

    updated = True
    while updated:
        updated = False
        for non_terminal, productions in grammar.items():
            for production in productions:
                for i, symbol in enumerate(production):
                    if symbol in non_terminals:

                        if i + 1 < len(production):
                            beta = production[i + 1:]
                            for first_symbol in first_sets[beta[0]]:
                                if first_symbol != 'ε':
                                    if first_symbol not in follow_sets[symbol]:
                                        follow_sets[symbol].add(first_symbol)
                                        updated = True
 
                            if 'ε' in compute_first_of_sequence(beta, first_sets):
                                for follow_b in follow_sets[non_terminal]:
                                    if follow_b not in follow_sets[symbol]:
                                        follow_sets[symbol].add(follow_b)
                                        updated = True

                        elif i == len(production) - 1:
                            for follow_b in follow_sets[non_terminal]:
                                if follow_b not in follow_sets[symbol]:
                                    follow_sets[symbol].add(follow_b)
                                    updated = True
    return follow_sets

def compute_first_of_sequence(symbols, first_sets):
    first = set()
    for symbol in symbols:
        first.update(first_sets[symbol] - {'ε'})
        if 'ε' not in first_sets[symbol]:
            return first
    first.add('ε')
    return first

if __name__ == '__main__':
    grammar_input = {
        'S': [['A', 'B']],
        'A': [['a'], ['ε']],
        'B': [['b']]
    }
    first_input = {
        'S': {'a', 'b'},
        'A': {'a', 'ε'},
        'B': {'b'},
        'a': {'a'},
        'b': {'b'}
    }
    non_terminals_input = {'S', 'A', 'B'}
    start_symbol_input = 'S'

    follow_result = compute_follow(grammar_input, first_input, non_terminals_input, start_symbol_input)
    print("FOLLOW sets:")
    for non_terminal, follow_set in follow_result.items():
        print(f"FOLLOW({non_terminal}) = {follow_set}")

    print("\n--- Another Example ---")
    grammar_input_2 = {
        'E': [['T', 'E\'']],
        'E\'': [['+', 'T', 'E\''], ['ε']],
        'T': [['F', 'T\'']],
        'T\'': [['*', 'F', 'T\''], ['ε']],
        'F': [['(', 'E', ')'], ['id']]
    }
    first_input_2 = {
        'E': {'(', 'id'},
        'E\'': {'+', 'ε'},
        'T': {'(', 'id'},
        'T\'': {'*', 'ε'},
        'F': {'(', 'id'},
        '+': {'+'},
        '*': {'*'},
        '(': {'('},
        ')': {')'},
        'id': {'id'}
    }
    non_terminals_input_2 = {'E', 'E\'', 'T', 'T\'', 'F'}
    start_symbol_input_2 = 'E'

    follow_result_2 = compute_follow(grammar_input_2, first_input_2, non_terminals_input_2, start_symbol_input_2)
    print("FOLLOW sets:")
    for non_terminal, follow_set in follow_result_2.items():
        print(f"FOLLOW({non_terminal}) = {follow_set}")

5. Program to implement LL(1) parser
FIRST and FOLLOW will be given, Parsing table is expected

# Given Grammar
grammar = {
    "E": ["T E'"],
    "E'": ["+ T E'", "ε"],
    "T": ["F T'"],
    "T'": ["* F T'", "ε"],
    "F": ["( E )", "id"]
}

# Given FIRST sets
first_sets = {
    "E": {"(", "id"},
    "E'": {"+", "ε"},
    "T": {"(", "id"},
    "T'": {"*", "ε"},
    "F": {"(", "id"}
}

# Given FOLLOW sets
follow_sets = {
    "E": {")", "$"},
    "E'": {")", "$"},
    "T": {"+", ")", "$"},
    "T'": {"+", ")", "$"},
    "F": {"*", "+", ")", "$"}
}

# Terminals and Non-terminals
non_terminals = list(grammar.keys())
terminals = {"id", "+", "*", "(", ")", "$"}

# Initialize parsing table
parsing_table = {nt: {t: "" for t in terminals} for nt in non_terminals}

# Helper: get FIRST of a string of symbols
def get_first_of_string(symbols):
    first = set()
    for symbol in symbols.split():
        if symbol not in first_sets:
            first.add(symbol)  # terminal
            break
        first |= (first_sets[symbol] - {"ε"})
        if "ε" not in first_sets[symbol]:
            break
    else:
        first.add("ε")
    return first

# Fill the LL(1) parsing table
for head in grammar:
    for production in grammar[head]:
        first = get_first_of_string(production)
        for terminal in (first - {"ε"}):
            parsing_table[head][terminal] = production
        if "ε" in first:
            for follow_symbol in follow_sets[head]:
                parsing_table[head][follow_symbol] = "ε"

# Print the parsing table
print("LL(1) Parsing Table:")
print(f"{'':<5}", end='')
for t in sorted(terminals):
    print(f"{t:<10}", end='')
print()

for nt in non_terminals:
    print(f"{nt:<5}", end='')
    for t in sorted(terminals):
        prod = parsing_table[nt][t]
        print(f"{prod:<10}", end='')
    print()






















6. Program to remove Left Recursion from the given grammar
Grammar will be given, modified grammar without left recursion is expected

def remove_left_recursion(grammar):
    new_grammar = {}

    for non_terminal in grammar:
        alpha = []  # left recursive productions (A → Aα)
        beta = []   # non-left recursive productions (A → β)

        for production in grammar[non_terminal]:
            if production.startswith(non_terminal):
                alpha.append(production[len(non_terminal):])  # Remove the NT from the left
            else:
                beta.append(production)

        # If left recursion exists
        if alpha:
            new_nt = non_terminal + "'"  # A' as new NT
            new_grammar[non_terminal] = [b + new_nt for b in beta]
            new_grammar[new_nt] = [a + new_nt for a in alpha]
            new_grammar[new_nt].append("ε")  # add epsilon
        else:
            new_grammar[non_terminal] = grammar[non_terminal]

    return new_grammar


# Example grammar with left recursion
grammar = {
    "A": ["Aa", "b"],
    "B": ["Bc", "Bd", "e"]
}

print("Original Grammar:")
for nt in grammar:
    print(f"{nt} -> {' | '.join(grammar[nt])}")

# Remove Left Recursion
updated_grammar = remove_left_recursion(grammar)

print("\nGrammar after removing Left Recursion:")
for nt in updated_grammar:
    print(f"{nt} -> {' | '.join(updated_grammar[nt])}")




7. Program to develop an Optimized Code.
Sample Code will be given, optimized code is expected

def constant_folding(code):
    optimized = []
    replacements = {}

    for line in code:
        parts = line.split('=')
        if len(parts) != 2:
            optimized.append(line)
            continue

        lhs = parts[0].strip()
        rhs = parts[1].strip()

        try:
            result = eval(rhs)
            replacements[lhs] = str(result)
            optimized.append(f"{lhs} = {result}")
        except:
            # Replace known constants in the RHS
            for var, val in replacements.items():
                rhs = rhs.replace(var, val)
            optimized.append(f"{lhs} = {rhs}")

    return optimized


def common_subexpression_elimination(code):
    expressions = {}
    optimized = []

    for line in code:
        if '=' not in line:
            optimized.append(line)
            continue

        lhs, rhs = [x.strip() for x in line.split('=')]

        # Skip replacement if RHS is a constant value
        try:
            eval(rhs)
            # it's a constant, don't replace with a variable
            optimized.append(f"{lhs} = {rhs}")
            continue
        except:
            pass

        if rhs in expressions:
            optimized.append(f"{lhs} = {expressions[rhs]}")
        else:
            expressions[rhs] = lhs
            optimized.append(f"{lhs} = {rhs}")

    return optimized


def optimize_code(code):
    print("\nStep 1: Constant Folding")
    code = constant_folding(code)
    for c in code:
        print(c)

    print("\nStep 2: Common Subexpression Elimination")
    code = common_subexpression_elimination(code)
    for c in code:
        print(c)

    return code


# 🔸 Sample input code
sample_code = [
    "t1 = 4 + 5",
    "t2 = a + b",
    "t3 = 4 + 5",
    "t4 = t1 + t2",
    "t5 = t3 + t2",
    "t6 = t1 + t2",
    "t7 = 2 * 3"
]

print("Original Code:")
for line in sample_code:
    print(line)

# 🔧 Optimize
final_code = optimize_code(sample_code)

print("\n🔹 Final Optimized Code:")
for line in final_code:
    print(line)



8. Program to implement Lexical Analyzer

import re

# Define token categories
keywords = {"int", "float", "char", "if", "else", "while", "for", "return", "void"}
operators = {'+', '-', '*', '/', '=', '==', '!=', '<', '>', '<=', '>=', '&&', '||'}
separators = {'(', ')', '{', '}', ';', ','}

# Classify each token
def classify_token(token):
    if token in keywords:
        return "Keyword"
    elif token in operators:
        return "Operator"
    elif token in separators:
        return "Separator"
    elif re.fullmatch(r'\d+(\.\d+)?', token):
        return "Literal"
    elif re.fullmatch(r'[a-zA-Z_]\w*', token):
        return "Identifier"
    else:
        return "Unknown"

# Lexical Analyzer
def lexical_analyzer(code):
    # Improved token pattern: float, int, words, multi-char ops, single-char ops/seps
    tokens = re.findall(r'\d+\.\d+|\d+|==|!=|<=|>=|[a-zA-Z_]\w*|[^\s]', code)

    token_types = {
        "Keyword": set(),
        "Identifier": set(),
        "Operator": set(),
        "Literal": set(),
        "Separator": set(),
        "Unknown": set()
    }

    for token in tokens:
        category = classify_token(token)
        token_types[category].add(token)

    # Print grouped results
    print("=== Lexical Analyzer Output (Grouped by Type) ===\n")
    for category in ["Keyword", "Identifier", "Operator", "Literal", "Separator", "Unknown"]:
        if token_types[category]:
            print(f"{category}s: {', '.join(sorted(token_types[category]))}")

# 🔸 Sample Code
sample_code = """
int a = 5;
float b = 3.14;
if (a > b) {
    a = a + b;
}
"""

lexical_analyzer(sample_code)






































9. LEX Program to check whether input character is vowel or consonants.

%{
#include <stdio.h>

int yywrap(void){
return 1;
}
%}

%%

[aeiouAEIOU]     { printf("'%s' is a VOWEL\n", yytext); }
[a-zA-Z]         { printf("'%s' is a CONSONANT\n", yytext); }
.|\n             { /* Ignore any other characters */ }

%%

int main() {
    printf("Enter a character: ");
    yylex();
    return 0;
}



Save the code in a file named vowel_consonant.l

lex vowel_consonant.l
gcc lex.yy.c -o vowel_consonant
./vowel_consonant


10. LEX Program to check whether the string is a word or a number.

%{
#include <stdio.h>
int yywrap(void);
%}

%%

[0-9]+(\.[0-9]+)?    { printf("'%s' is a NUMBER\n", yytext); }
[a-zA-Z]+            { printf("'%s' is a WORD\n", yytext); }
.|\n                 { /* Ignore other characters */ }

%%

int yywrap(void) {
    return 1;
}

int main() {
    printf("Enter input: ");
    yylex();
    return 0;
}




11. EX Program to count the number of lines, words and characters in a text.

%{
#include <stdio.h>

int line_count = 0;
int word_count = 0;
int char_count = 0;

int yywrap(void);
%}

%%

\n              { line_count++; char_count++; }
[ \t]+          { char_count += yyleng; }                     // Spaces and tabs (word separators)
[a-zA-Z0-9]+    { word_count++; char_count += yyleng; }       // Words (letters/numbers)
.               { char_count += yyleng; }                     // All other characters

%%

int yywrap(void) {
    return 1;
}

int main() {
    printf("Enter text (Ctrl+D to end input):\n");
    yylex();
    printf("\nLines: %d\nWords: %d\nCharacters: %d\n", line_count, word_count, char_count);
    return 0;
}




12. LEX Program to implement Lexical Analyzer

%{
#include <stdio.h>
#include <string.h>

char *keywords[] = {
    "int", "float", "char", "double", "if", "else", "while", "for", "return", "void"
};
int isKeyword(char *str) {
    for(int i = 0; i < sizeof(keywords)/sizeof(keywords[0]); i++) {
        if(strcmp(str, keywords[i]) == 0)
            return 1;
    }
    return 0;
}
%}

%%
[ \t\n]                  ; // Ignore whitespace
"=="|"!="|"<="|">="|"="|"<"|">"    { printf("Relational Operator: %s\n", yytext); }
"*"|"+"|"-"|"/"          { printf("Arithmetic Operator: %s\n", yytext); }
[0-9]+\.[0-9]+           { printf("Float Literal: %s\n", yytext); }
[0-9]+                   { printf("Integer Literal: %s\n", yytext); }
[a-zA-Z_][a-zA-Z0-9_]*   {
                            if (isKeyword(yytext))
                                printf("Keyword: %s\n", yytext);
                            else
                                printf("Identifier: %s\n", yytext);
                         }
.                        { printf("Special Symbol: %s\n", yytext); }
%%

int main() {
    printf("Enter the input:\n");
    yylex();
    return 0;
}

int yywrap() {
    return 1;
}





